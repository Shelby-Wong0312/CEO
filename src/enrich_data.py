"""
enrich_data.py - è³‡æ–™æ“´å……è…³æœ¬ (Executive Search & Private Investigator Quality)

æ ¹æ“š Standard Example.xlsx ä¸­çš„å§“åèˆ‡å…¬å¸è³‡è¨Šï¼Œ
ä½¿ç”¨å¤šé‡æœå°‹ç­–ç•¥å¡«è£œç©ºç¼ºæ¬„ä½ã€‚

å‡ç´šé‡é» (Phase 31 - ä¿®å¾©ç‰ˆ):
1. ä¿®å¾© Excel è®€å– dtype å•é¡Œï¼ˆURL è¢«èª¤åˆ¤ç‚º float64ï¼‰
2. æ”¹é€² DuckDuckGo æœå°‹éŒ¯èª¤è™•ç†
3. å¢åŠ ç¶²è·¯é€£ç·šæ¸¬è©¦

ä½¿ç”¨æ–¹å¼:
    python src/enrich_data.py --rows "2, 5-10, 15"
"""

import sys
import io

# è¨­å®šæ¨™æº–è¼¸å‡ºç‚º UTF-8
if sys.stdout.encoding != 'utf-8':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

import argparse
import os
import re
import json
import time
from pathlib import Path

import pandas as pd
import requests
from dotenv import load_dotenv

# å°å…¥çµ±ä¸€æœå°‹å®¢æˆ¶ç«¯
try:
    from src.search import UnifiedSearchClient
    UNIFIED_SEARCH_AVAILABLE = True
except ImportError:
    UNIFIED_SEARCH_AVAILABLE = False

# å˜—è©¦å°å…¥ DuckDuckGo æœå°‹ï¼ˆä½œç‚º fallbackï¼‰
try:
    from duckduckgo_search import DDGS
    DDGS_AVAILABLE = True
except ImportError:
    try:
        from ddgs import DDGS
        DDGS_AVAILABLE = True
    except ImportError:
        DDGS_AVAILABLE = False
        if not UNIFIED_SEARCH_AVAILABLE:
            print("è­¦å‘Š: ddgs æœªå®‰è£ï¼Œå°‡åƒ…ä½¿ç”¨ Perplexity API")

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# === å¸¸æ•¸å®šç¾© ===
EXCEL_INPUT = "Standard Example.xlsx"
EXCEL_OUTPUT = "output/data/Standard_Example_Enriched.xlsx"
PHOTO_CANDIDATES_JSON = "output/data/photo_candidates.json"
PHOTO_REVIEW_HTML = "output/data/photo_review.html"

# ç…§ç‰‡ä¿¡å¿ƒåº¦é–€æª»ï¼ˆåˆ†æ•¸ >= æ­¤å€¼æ‰è‡ªå‹•å¡«å…¥ï¼‰
PHOTO_CONFIDENCE_THRESHOLD = 30

# éœ€è¦æ“´å……çš„æ¬„ä½ï¼ˆå°æ‡‰ Excel æ¬„ä½åç¨±ï¼‰
ENRICHABLE_COLUMNS = [
    "å¹´é½¡",
    "ç…§ç‰‡",
    "ç…§ç‰‡ç‹€æ…‹",
    "å°ˆæ¥­åˆ†é¡",
    "å°ˆæ¥­èƒŒæ™¯",
    "å­¸æ­·",
    "ä¸»è¦ç¶“æ­·",
    "ç¾è·/ä»»",
    "å€‹äººç‰¹è³ª",
    "ç¾æ“”ä»»ç¨è‘£å®¶æ•¸(å¹´)",
    "æ“”ä»»ç¨è‘£å¹´è³‡(å¹´)",
    "é›»å­éƒµä»¶",
    "å…¬å¸é›»è©±"
]

# API å›å‚³æ¬„ä½å°æ‡‰åˆ° Excel æ¬„ä½
API_TO_EXCEL_MAPPING = {
    "age": "å¹´é½¡",
    "photo_url": "ç…§ç‰‡",
    "professional_category": "å°ˆæ¥­åˆ†é¡",
    "professional_background": "å°ˆæ¥­èƒŒæ™¯",
    "education": "å­¸æ­·",
    "key_experience": "ä¸»è¦ç¶“æ­·",
    "current_position": "ç¾è·/ä»»",
    "personal_traits": "å€‹äººç‰¹è³ª",
    "independent_director_count": "ç¾æ“”ä»»ç¨è‘£å®¶æ•¸(å¹´)",
    "independent_director_tenure": "æ“”ä»»ç¨è‘£å¹´è³‡(å¹´)",
    "email": "é›»å­éƒµä»¶",
    "phone": "å…¬å¸é›»è©±"
}

# éœ€è¦çµæ§‹åŒ–è¼¸å‡ºçš„æ¬„ä½ï¼ˆä½¿ç”¨æ›è¡Œåˆ†éš”ï¼‰
STRUCTURED_FIELDS = ["å­¸æ­·", "ä¸»è¦ç¶“æ­·", "ç¾è·/ä»»", "å€‹äººç‰¹è³ª"]


def test_network_connection() -> bool:
    """
    æ¸¬è©¦ç¶²è·¯é€£ç·šæ˜¯å¦æ­£å¸¸ã€‚
    
    Returns:
        True å¦‚æœç¶²è·¯æ­£å¸¸
    """
    test_urls = [
        "https://www.google.com",
        "https://duckduckgo.com",
        "https://www.bing.com"
    ]
    
    for url in test_urls:
        try:
            response = requests.get(url, timeout=5)
            if response.status_code == 200:
                return True
        except:
            continue
    
    return False


def read_excel_safe(filepath: str) -> pd.DataFrame:
    """
    å®‰å…¨è®€å– Excel æª”æ¡ˆï¼Œé¿å… dtype å•é¡Œã€‚
    
    Args:
        filepath: Excel æª”æ¡ˆè·¯å¾‘
        
    Returns:
        DataFrame
    """
    # å®šç¾©æ‰€æœ‰å¯èƒ½åŒ…å«å­—ä¸²çš„æ¬„ä½ç‚º str é¡å‹
    dtype_spec = {
        "å§“åï¼ˆä¸­è‹±ï¼‰": str,
        "æ‰€å±¬å…¬å¸": str,
        "å¹´é½¡": str,
        "ç…§ç‰‡": str,
        "ç…§ç‰‡ç‹€æ…‹": str,
        "å°ˆæ¥­åˆ†é¡": str,
        "å°ˆæ¥­èƒŒæ™¯": str,
        "å­¸æ­·": str,
        "ä¸»è¦ç¶“æ­·": str,
        "ç¾è·/ä»»": str,
        "å€‹äººç‰¹è³ª": str,
        "ç¾æ“”ä»»ç¨è‘£å®¶æ•¸(å¹´)": str,
        "æ“”ä»»ç¨è‘£å¹´è³‡(å¹´)": str,
        "é›»å­éƒµä»¶": str,
        "å…¬å¸é›»è©±": str
    }
    
    try:
        # å…ˆå˜—è©¦ç”¨æŒ‡å®š dtype è®€å–
        df = pd.read_excel(filepath, dtype=dtype_spec)
        return df
    except Exception as e1:
        print(f"    æ³¨æ„: ä½¿ç”¨æŒ‡å®š dtype è®€å–å¤±æ•—ï¼Œå˜—è©¦è‡ªå‹•åµæ¸¬...")
        try:
            # é€€å›åˆ°è‡ªå‹•åµæ¸¬ï¼Œä½†ä¹‹å¾Œè½‰æ›æ¬„ä½é¡å‹
            df = pd.read_excel(filepath)
            
            # å°‡æ‰€æœ‰ ENRICHABLE_COLUMNS è½‰ç‚º object é¡å‹
            for col in df.columns:
                if col in ENRICHABLE_COLUMNS or col in ["å§“åï¼ˆä¸­è‹±ï¼‰", "æ‰€å±¬å…¬å¸"]:
                    df[col] = df[col].astype(object)
            
            return df
        except Exception as e2:
            raise Exception(f"è®€å– Excel å¤±æ•—: {e2}")


def parse_row_numbers(rows_str: str) -> list[int]:
    """è§£æ --rows åƒæ•¸å­—ä¸²ï¼Œè½‰æ›ç‚º Excel åˆ—è™Ÿåˆ—è¡¨ã€‚"""
    rows_str = rows_str.replace('ï¼Œ', ',')
    result = set()
    parts = rows_str.replace(" ", "").split(",")

    for part in parts:
        if not part:
            continue
        if "-" in part:
            match = re.match(r"^(\d+)-(\d+)$", part)
            if match:
                start, end = int(match.group(1)), int(match.group(2))
                if start > end:
                    start, end = end, start
                result.update(range(start, end + 1))
            else:
                print(f"è­¦å‘Š: ç„¡æ³•è§£æç¯„åœ '{part}'ï¼Œå·²è·³é")
        else:
            try:
                result.add(int(part))
            except ValueError:
                print(f"è­¦å‘Š: ç„¡æ³•è§£ææ•¸å­— '{part}'ï¼Œå·²è·³é")

    result.discard(1)
    return sorted(result)


def excel_row_to_pandas_index(excel_row: int) -> int:
    """å°‡ Excel åˆ—è™Ÿè½‰æ›ç‚º pandas DataFrame ç´¢å¼•ã€‚"""
    return excel_row - 2


def search_with_ddg(query: str, max_results: int = 5) -> list[dict]:
    """
    ä½¿ç”¨ DuckDuckGo é€²è¡Œç¶²è·¯æœå°‹ï¼ˆå¢å¼·éŒ¯èª¤è™•ç†ç‰ˆï¼‰ã€‚

    Returns:
        æœå°‹çµæœåˆ—è¡¨ï¼Œæ¯å€‹çµæœåŒ…å« title, href, body
    """
    if not DDGS_AVAILABLE:
        return []

    max_retries = 3
    for attempt in range(max_retries):
        try:
            with DDGS() as ddgs:
                results = list(ddgs.text(query, max_results=max_results, region='tw-tzh'))
                return results
        except Exception as e:
            error_msg = str(e).lower()
            
            # åˆ¤æ–·éŒ¯èª¤é¡å‹
            if 'ratelimit' in error_msg or 'rate' in error_msg:
                print(f"    DuckDuckGo è«‹æ±‚éæ–¼é »ç¹ï¼Œç­‰å¾… {(attempt + 1) * 5} ç§’...")
                time.sleep((attempt + 1) * 5)
            elif 'timeout' in error_msg:
                print(f"    DuckDuckGo é€£ç·šé€¾æ™‚ï¼Œé‡è©¦ä¸­ ({attempt + 1}/{max_retries})...")
                time.sleep(2)
            elif 'no results' in error_msg:
                # æ²’æœ‰çµæœä¸æ˜¯éŒ¯èª¤ï¼Œç›´æ¥è¿”å›ç©ºåˆ—è¡¨
                return []
            else:
                print(f"    DuckDuckGo æœå°‹éŒ¯èª¤ ({attempt + 1}/{max_retries}): {e}")
                time.sleep(2)
    
    return []


def extract_linkedin_url(results: list[dict]) -> str:
    """å¾æœå°‹çµæœä¸­æå– LinkedIn URLã€‚"""
    for result in results:
        href = result.get('href', '')
        if 'linkedin.com/in/' in href:
            return href
    return ""


def score_image_result(result: dict, name: str, company: str) -> int:
    """
    ç‚ºåœ–ç‰‡æœå°‹çµæœè©•åˆ†ï¼Œåˆ†æ•¸è¶Šé«˜è¶Šå¯é ã€‚
    """
    score = 0
    image_url = result.get('image', '').lower()
    source_url = result.get('url', '').lower()
    title = result.get('title', '').lower()
    width = result.get('width', 0)
    height = result.get('height', 0)

    # === ä¾†æºè©•åˆ† ===
    if 'linkedin.com' in source_url or 'linkedin' in image_url:
        score += 50

    company_domain_hints = ['company', 'corporate', 'about', 'team', 'leadership', 'management']
    if any(hint in source_url for hint in company_domain_hints):
        score += 40

    news_sites = ['reuters', 'bloomberg', 'forbes', 'businessweek', 'cna.com', 'udn.com',
                  'ltn.com', 'chinatimes', 'ettoday', 'setn.com', 'bnext', 'technews']
    if any(site in source_url for site in news_sites):
        score += 20

    # === åœ–ç‰‡å°ºå¯¸è©•åˆ† ===
    if width > 0 and height > 0:
        if width >= 150 and height >= 150:
            score += 15
        aspect_ratio = width / height if height > 0 else 0
        if 0.6 <= aspect_ratio <= 1.2:
            score += 10
        if aspect_ratio > 2.0:
            score -= 20

    # === URL/æ¨™é¡ŒåŒ…å«äººå ===
    name_parts = name.lower().split()
    for part in name_parts:
        if len(part) > 1 and part in image_url:
            score += 10
            break
        if len(part) > 1 and part in title:
            score += 5
            break

    # === æ’é™¤ä¸è‰¯ä¾†æº ===
    bad_keywords = ['logo', 'icon', 'banner', 'placeholder', 'avatar', 'default',
                    'stock', 'shutterstock', 'istockphoto', 'gettyimages', 'dreamstime',
                    'thumbnail', 'sprite', 'emoji', 'badge', 'button']
    if any(bad in image_url for bad in bad_keywords):
        score -= 100

    if 'default' in image_url and ('profile' in image_url or 'avatar' in image_url):
        score -= 100

    return score


def validate_image_url(url: str) -> bool:
    """é©—è­‰åœ–ç‰‡ URL æ˜¯å¦æœ‰æ•ˆã€‚"""
    if not url:
        return False

    lower_url = url.lower()
    valid_extensions = ['.jpg', '.jpeg', '.png', '.webp', '.gif']
    has_valid_ext = any(ext in lower_url for ext in valid_extensions)

    if not has_valid_ext:
        try:
            headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
            response = requests.head(url, headers=headers, timeout=5, allow_redirects=True)
            content_type = response.headers.get('Content-Type', '')
            if not content_type.startswith('image/'):
                return False
        except:
            pass

    return True


def find_executive_photo_python(name: str, company: str, job_title: str = "") -> dict:
    """
    ä½¿ç”¨å¤šé‡ç­–ç•¥æœå°‹é«˜éšä¸»ç®¡ç…§ç‰‡ URLï¼ˆå¢å¼·éŒ¯èª¤è™•ç†ç‰ˆï¼‰ã€‚
    """
    result = {
        "best_url": "",
        "best_score": 0,
        "status": "å¾…è£œå……",
        "candidates": []
    }

    if not DDGS_AVAILABLE:
        print("    [ç…§ç‰‡æœå°‹] DuckDuckGo æœªå®‰è£ï¼Œè·³é")
        return result

    # å…ˆæ¸¬è©¦ç¶²è·¯é€£ç·š
    if not test_network_connection():
        print("    [ç…§ç‰‡æœå°‹] ç¶²è·¯é€£ç·šç•°å¸¸ï¼Œè·³é")
        return result

    # === å»ºç«‹å¤šé‡æœå°‹æŸ¥è©¢ ===
    search_queries = []
    search_queries.append(f'site:linkedin.com "{name}" {company}')
    
    if job_title:
        first_title = job_title.split('\n')[0].strip()
        if first_title:
            search_queries.append(f'"{name}" "{first_title}" photo OR portrait')

    search_queries.append(f'"{name}" {company} ç…§ç‰‡ OR headshot OR portrait')
    search_queries.append(f'{name} {company} profile photo')

    # === æ”¶é›†æ‰€æœ‰æœå°‹çµæœ ===
    all_results = []
    seen_urls = set()

    print(f"    [ç…§ç‰‡æœå°‹] åŸ·è¡Œ {len(search_queries)} ç¨®æœå°‹ç­–ç•¥...")

    try:
        with DDGS() as ddgs:
            for i, query in enumerate(search_queries, 1):
                try:
                    print(f"    [ç­–ç•¥ {i}] {query[:50]}...")
                    results = list(ddgs.images(query, max_results=5))

                    if not results:
                        print(f"    [ç­–ç•¥ {i}] æœå°‹å¤±æ•—: No results found.")
                        continue

                    for img in results:
                        image_url = img.get('image', '')
                        if image_url and image_url not in seen_urls:
                            seen_urls.add(image_url)
                            if validate_image_url(image_url):
                                all_results.append(img)

                    time.sleep(1)  # å¢åŠ é–“éš”é¿å… rate limit

                except Exception as e:
                    error_msg = str(e).lower()
                    if 'no results' in error_msg or 'empty' in error_msg:
                        print(f"    [ç­–ç•¥ {i}] æœå°‹å¤±æ•—: No results found.")
                    elif 'ratelimit' in error_msg or 'rate' in error_msg:
                        print(f"    [ç­–ç•¥ {i}] æœå°‹å¤±æ•—: Rate limited, ç­‰å¾…ä¸­...")
                        time.sleep(5)
                    else:
                        print(f"    [ç­–ç•¥ {i}] æœå°‹å¤±æ•—: {e}")
                    continue

    except Exception as e:
        print(f"    [ç…§ç‰‡æœå°‹] DuckDuckGo éŒ¯èª¤: {e}")
        return result

    if not all_results:
        print("    [ç…§ç‰‡æœå°‹] æœªæ‰¾åˆ°ä»»ä½•çµæœ")
        return result

    print(f"    [ç…§ç‰‡æœå°‹] æ”¶é›†åˆ° {len(all_results)} å¼µå€™é¸åœ–ç‰‡ï¼Œè©•åˆ†ä¸­...")

    # === è©•åˆ†ä¸¦æ’åº ===
    scored_results = []
    for img_result in all_results:
        score = score_image_result(img_result, name, company)
        if score > -50:
            scored_results.append((score, img_result))

    if not scored_results:
        print("    [ç…§ç‰‡æœå°‹] æ‰€æœ‰å€™é¸åœ–ç‰‡è©•åˆ†éä½")
        return result

    scored_results.sort(key=lambda x: x[0], reverse=True)

    print(f"    [ç…§ç‰‡æœå°‹] å€™é¸åœ–ç‰‡è©•åˆ†:")
    for i, (score, img_result) in enumerate(scored_results[:3], 1):
        url = img_result.get('image', '')[:50]
        source = img_result.get('url', '')[:30]
        print(f"      #{i} åˆ†æ•¸:{score:3d} | {url}... | ä¾†æº:{source}...")

    for score, img_result in scored_results[:5]:
        result["candidates"].append({
            "url": img_result.get('image', ''),
            "score": score,
            "source": img_result.get('url', ''),
            "title": img_result.get('title', ''),
            "width": img_result.get('width', 0),
            "height": img_result.get('height', 0)
        })

    best_score, best_img = scored_results[0]
    best_url = best_img.get('image', '')
    result["best_score"] = best_score

    if best_score >= PHOTO_CONFIDENCE_THRESHOLD:
        result["best_url"] = best_url
        result["status"] = "å¾…ç¢ºèª"
        print(f"    [ç…§ç‰‡æœå°‹] âœ“ åˆ†æ•¸ {best_score} >= {PHOTO_CONFIDENCE_THRESHOLD}ï¼Œè‡ªå‹•å¡«å…¥ï¼ˆå¾…ç¢ºèªï¼‰")
    else:
        result["best_url"] = ""
        result["status"] = "å¾…è£œå……"
        print(f"    [ç…§ç‰‡æœå°‹] âœ— åˆ†æ•¸ {best_score} < {PHOTO_CONFIDENCE_THRESHOLD}ï¼Œéœ€äººå·¥å¯©æ ¸")

    return result


def extract_info_from_snippets(results: list[dict], name: str) -> dict:
    """å¾æœå°‹çµæœçš„æ‘˜è¦ä¸­æå–è³‡è¨Šã€‚"""
    extracted = {}
    linkedin = extract_linkedin_url(results)
    if linkedin:
        extracted['LinkedIn'] = linkedin
    return extracted


def build_executive_search_prompt(name: str, company: str) -> str:
    """å»ºç«‹ Executive Search Researcher & Private Investigator å“è³ªçš„æœå°‹æç¤ºè©ã€‚"""
    import datetime
    current_year = datetime.datetime.now().year

    prompt = f"""# Role
You are an elite Executive Search Researcher & Private Investigator (é«˜éšçµé ­èˆ‡å¾µä¿¡å°ˆå®¶).
Your mission is to construct a **complete, verified profile** for the target executive.
You do not give up easily. You dig deep, infer intelligently, and verify strictly.

# Target Executive
Name: {name}
Company: {company}

# Search & Inference Protocol (MUST FOLLOW)

## 1. The "Age" Heuristic (Critical)
**Problem:** Direct "Age" is often missing.
**Solution:** You MUST attempt to **calculate** it if not found directly.
- **Step A:** Search for "Bachelor's degree year" or "University graduation year".
    - *Formula:* If they graduated Bachelor's in 1990 -> 1990 + 22 = Born approx 1968 -> Current Age = {current_year} - 1968.
- **Step B:** Search for "Date of birth" or "Born in".
- **Step C:** Search for old news (e.g., "Appointed in 2015 at age 45" -> Current Age = 45 + ({current_year} - 2015)).
- **Output:** Return the number (e.g., "55æ­²") only if you have a grounded estimation. Otherwise, null.

## 2. The "Education" Deep Dive
- Ignore generic bios. Search specifically for:
    - `"{name}" "{company}" education`
    - `"{name}" "{company}" alumni`
    - `"{name}" "{company}" LinkedIn`
    - `"{name}" "{company}" ç•¢æ¥­`
- **Requirement:** Must list Degree + School (e.g., "åœ‹ç«‹å°ç£å¤§å­¸ é›»æ©Ÿç³» å­¸å£«").

## 3. CONTACT INFO: The "Zero-Fail" Zone (Strict Rules)
- **Email & Phone:** These are **High-Risk Fields**.
- **Rule:** You are FORBIDDEN from guessing. You are FORBIDDEN from constructing emails like `name@company.com` unless you find it indexed on the web.
- **Search Targets:** Look for PDF presentations, conference attendee lists, or official press contacts.
- **Verification:** If you find `info@company.com`, IGNORE IT. Only personal work emails (e.g., `john.doe@company.com`) are accepted.
- **Output:** If 100% sure, return the string. If 99% sure, return `""`. **Accuracy > Availability.**

## 4. Professional Category (å°ˆæ¥­åˆ†é¡) - REQUIRED
Classify the person into ONE of the following categories based on their PRIMARY expertise:

**Categories (MUST choose exactly ONE):**
- "æœƒè¨ˆ/è²¡å‹™é¡" - For: æœƒè¨ˆå¸«ã€è²¡å‹™é•·ã€CFOã€è²¡æœƒå­¸è€…ã€å¯©è¨ˆå¸«
- "æ³•å‹™é¡" - For: å¾‹å¸«ã€æ³•å®˜ã€æª¢å¯Ÿå®˜ã€æ³•å­¸æ•™æˆã€æ³•å‹™é•·
- "å•†å‹™/ç®¡ç†é¡" - For: ä¼æ¥­ç¶“ç‡Ÿè€…ã€ç®¡ç†å­¸è€…ã€å•†å­¸é™¢æ•™æˆã€CEOã€ç¸½ç¶“ç†ã€è‘£äº‹é•·
- "ç”¢æ¥­å°ˆæ¥­é¡" - For: å·¥ç¨‹å¸«ã€æŠ€è¡“å°ˆå®¶ã€ç§‘æŠ€æ¥­ä¸»ç®¡ã€é‡‘èå°ˆæ¥­äººå“¡ã€é†«ç™‚å°ˆæ¥­ç­‰
- "å…¶ä»–å°ˆé–€è·æ¥­" - For: å»ºç¯‰å¸«ã€æŠ€å¸«ã€åœ‹è€ƒåŠæ ¼ä¹‹å°ˆæ¥­äººå“¡

## 5. The "Professional Background" Summary (å°ˆæ¥­èƒŒæ™¯)
This is a ONE-PARAGRAPH executive summary of the person's expertise.
**Format (REQUIRED):**
"ç´„ X å¹´åœ¨[ç”¢æ¥­1]ã€[ç”¢æ¥­2]ã€[ç”¢æ¥­3]ç­‰é ˜åŸŸç¶“æ­·ï¼Œå°ˆé•·æ–¼[å°ˆæ¥­é ˜åŸŸ]ï¼Œé•·æœŸåœ¨[å…¬å¸é¡å‹]æ“”ä»»[è·ä½å±¤ç´š]è·å‹™ã€‚"

## 6. The "Personal Traits" Analysis (å€‹äººç‰¹è³ª) - MUST BE DETAILED
**Format (STRICTLY REQUIRED):**
Return a SINGLE STRING with numbered items:
"1.[ç‰¹è³ªåç¨±]\\n- [å…·é«”äº‹è¹Ÿæˆ–æè¿°]\\n2.[ç‰¹è³ªåç¨±]\\n- [å…·é«”äº‹è¹Ÿæˆ–æè¿°]\\n3.[ç‰¹è³ªåç¨±]\\n- [å…·é«”äº‹è¹Ÿæˆ–æè¿°]"

## 7. Independent Director Stats
- Search for "{name} ç¨ç«‹è‘£äº‹" or "{name} ç¨è‘£ å¹´è³‡".
- If found, return count and tenure. Otherwise, null.

# Output Format
Return **ONLY** a raw JSON object (no markdown, no extra text):
{{
  "company_industry": "String (å…¬å¸ç”¢æ¥­åˆ¥)",
  "chamber_of_commerce": "String (æ‰€å±¬å•†æœƒ/å”æœƒ)",
  "age": "String (e.g. '54æ­²') or null",
  "professional_category": "String (æœƒè¨ˆ/è²¡å‹™é¡ | æ³•å‹™é¡ | å•†å‹™/ç®¡ç†é¡ | ç”¢æ¥­å°ˆæ¥­é¡ | å…¶ä»–å°ˆé–€è·æ¥­)",
  "professional_background": "String (ç´„ X å¹´åœ¨[é ˜åŸŸ]ç¶“æ­·...)",
  "education": ["String (å­¸æ ¡ ç§‘ç³» å­¸ä½)", ...],
  "key_experience": ["String (å…¬å¸: è·ä½ (æˆå°±/åœ°å€))", ...],
  "current_position": ["String (ç¾ä»»è·ä½)", ...],
  "personal_traits": "String (1.ç‰¹è³ªä¸€\\n- å…·é«”æè¿°\\n2.ç‰¹è³ªäºŒ\\n- å…·é«”æè¿°)",
  "independent_director_count": Integer or null,
  "independent_director_tenure": "String (e.g. '5å¹´') or null",
  "email": "String or null (STRICT: 100% verified only)",
  "phone": "String or null (STRICT: 100% verified only)",
  "photo_search_term": "String (æœ€ä½³åœ–ç‰‡æœå°‹é—œéµå­—)"
}}

CRITICAL REMINDERS:
1. Age: Use the heuristic formula if direct age is not found.
2. Contact: Return "" if not 100% verified. Never guess.
3. All text in Traditional Chinese (ç¹é«”ä¸­æ–‡) for the final output.
4. Return ONLY the JSON object. No markdown, no explanations."""

    return prompt


def _clean_value(value) -> str:
    """æ¸…ç†æ¬„ä½å€¼ï¼Œå°‡ nullã€NaNã€placeholder ç­‰ç„¡æ•ˆå€¼è½‰ç‚ºç©ºå­—ä¸²ã€‚"""
    if value is None:
        return ""

    if isinstance(value, float):
        import math
        if math.isnan(value):
            return ""

    str_value = str(value).strip()

    if not str_value:
        return ""

    placeholder_values = [
        "null", "none", "nan", "n/a", "na", "undefined",
        "å·²ç•¥é", "å¾…è£œå……", "(å¾…è£œå……)", "ï¼ˆå¾…è£œå……ï¼‰",
        "ç„¡", "ç„¡è³‡æ–™", "æ‰¾ä¸åˆ°", "æœªçŸ¥", "ä¸æ˜",
        "æš«ç„¡", "å°šç„¡", "ç¼º", "ç©º", "nil"
    ]

    if str_value.lower() in [p.lower() for p in placeholder_values]:
        return ""

    skip_prefixes = ["ç„¡æ³•", "æ‰¾ä¸åˆ°", "æŸ¥ç„¡", "å°šæœª", "æš«ç„¡æ³•"]
    for prefix in skip_prefixes:
        if str_value.startswith(prefix):
            return ""

    return str_value


def _is_valid_age(age_str: str, professional_background: str = None) -> bool:
    """é©—è­‰å¹´é½¡æ˜¯å¦åˆç†ã€‚"""
    if not age_str:
        return False

    age_match = re.search(r'(\d+)', str(age_str))
    if not age_match:
        return False

    age = int(age_match.group(1))

    if age < 35 or age > 85:
        return False

    if professional_background:
        years_match = re.search(r'ç´„\s*(\d+)\s*å¹´', professional_background)
        if years_match:
            experience_years = int(years_match.group(1))
            min_age_required = 22 + experience_years
            if age < min_age_required:
                return False

    return True


def _extract_experience_years(professional_background: str) -> int:
    """å¾å°ˆæ¥­èƒŒæ™¯ä¸­æå–å·¥ä½œå¹´è³‡ã€‚"""
    if not professional_background:
        return 0

    years_match = re.search(r'ç´„\s*(\d+)\s*å¹´', professional_background)
    if years_match:
        return int(years_match.group(1))
    return 0


def _is_valid_education_entry(text: str) -> bool:
    """é©—è­‰å­¸æ­·æ¢ç›®æ˜¯å¦ç‚ºæœ‰æ•ˆæ ¼å¼ã€‚"""
    if not text or not isinstance(text, str):
        return False

    text = text.strip()

    if len(text) > 100:
        return False

    garbage_patterns = [
        r'\d+\s*(day|hour|minute|second)s?\s*ago',
        r'\d+\s*(å¤©|å°æ™‚|åˆ†é˜)å‰',
        r'Â·',
        r'ã€‹',
        r'ã€Š',
        r'http[s]?://',
        r'ç¸½ç¶“ç†',
        r'è‘£äº‹é•·',
        r'åŸ·è¡Œé•·',
        r'CEO',
    ]

    for pattern in garbage_patterns:
        if re.search(pattern, text, re.IGNORECASE):
            return False

    edu_keywords = [
        'å¤§å­¸', 'å­¸é™¢', 'ç ”ç©¶æ‰€', 'å­¸ç³»', 'ç³»',
        'å­¸å£«', 'ç¢©å£«', 'åšå£«', 'ç•¢æ¥­',
        'University', 'College', 'Institute', 'School',
        'Bachelor', 'Master', 'MBA', 'EMBA', 'PhD', 'Doctor',
    ]

    has_edu_keyword = any(kw in text for kw in edu_keywords)
    if not has_edu_keyword:
        return False

    if len(text) < 5:
        return False

    return True


def process_api_response(api_data: dict) -> dict:
    """å°‡ API å›å‚³çš„è³‡æ–™è½‰æ›ç‚º Excel æ¬„ä½æ ¼å¼ã€‚"""
    result = {}

    professional_background = None
    if api_data.get("professional_background"):
        bg = api_data["professional_background"]
        if isinstance(bg, str) and bg.strip():
            professional_background = bg.strip()
            result["å°ˆæ¥­èƒŒæ™¯"] = professional_background

    if api_data.get("age"):
        age_str = str(api_data["age"])
        if _is_valid_age(age_str, professional_background):
            result["å¹´é½¡"] = age_str

    if api_data.get("professional_category"):
        cat = api_data["professional_category"]
        if isinstance(cat, str) and cat.strip():
            valid_categories = ["æœƒè¨ˆ/è²¡å‹™é¡", "æ³•å‹™é¡", "å•†å‹™/ç®¡ç†é¡", "ç”¢æ¥­å°ˆæ¥­é¡", "å…¶ä»–å°ˆé–€è·æ¥­"]
            cat_clean = cat.strip()
            if cat_clean in valid_categories:
                result["å°ˆæ¥­åˆ†é¡"] = cat_clean
            else:
                for valid_cat in valid_categories:
                    if valid_cat in cat_clean or cat_clean in valid_cat:
                        result["å°ˆæ¥­åˆ†é¡"] = valid_cat
                        break

    if api_data.get("education"):
        edu = api_data["education"]
        if isinstance(edu, list):
            valid_edu = []
            for item in edu:
                if isinstance(item, str) and _is_valid_education_entry(item):
                    valid_edu.append(item.strip())
            if valid_edu:
                result["å­¸æ­·"] = "\n".join(valid_edu)
        elif isinstance(edu, str) and _is_valid_education_entry(edu):
            result["å­¸æ­·"] = edu.strip()

    if api_data.get("key_experience"):
        exp = api_data["key_experience"]
        if isinstance(exp, list):
            result["ä¸»è¦ç¶“æ­·"] = "\n".join(exp)
        else:
            result["ä¸»è¦ç¶“æ­·"] = str(exp)

    if api_data.get("current_position"):
        pos = api_data["current_position"]
        if isinstance(pos, list):
            result["ç¾è·/ä»»"] = "\n".join(pos)
        else:
            result["ç¾è·/ä»»"] = str(pos)

    if api_data.get("personal_traits"):
        traits = api_data["personal_traits"]
        if isinstance(traits, list):
            result["å€‹äººç‰¹è³ª"] = "\n".join(traits)
        else:
            result["å€‹äººç‰¹è³ª"] = str(traits)

    if api_data.get("independent_director_count") is not None:
        result["ç¾æ“”ä»»ç¨è‘£å®¶æ•¸(å¹´)"] = str(api_data["independent_director_count"])

    if api_data.get("independent_director_tenure"):
        result["æ“”ä»»ç¨è‘£å¹´è³‡(å¹´)"] = str(api_data["independent_director_tenure"])

    email = api_data.get("email")
    if email and isinstance(email, str) and "@" in email and email.lower() not in ["", "null", "none"]:
        generic_patterns = ["info@", "contact@", "service@", "support@", "admin@", "hello@"]
        is_generic = any(pattern in email.lower() for pattern in generic_patterns)
        if not is_generic:
            result["é›»å­éƒµä»¶"] = email

    phone = api_data.get("phone")
    if phone and isinstance(phone, str) and phone.lower() not in ["", "null", "none"]:
        if re.search(r'\d{6,}', phone.replace("-", "").replace(" ", "")):
            result["å…¬å¸é›»è©±"] = phone

    if api_data.get("photo_search_term"):
        result["_photo_search_term"] = api_data["photo_search_term"]

    cleaned_result = {}
    for key, value in result.items():
        if key.startswith("_"):
            cleaned_result[key] = value
            continue

        if isinstance(value, str):
            cleaned_value = _clean_value(value)
            if cleaned_value:
                cleaned_result[key] = cleaned_value
        elif value is not None:
            cleaned_result[key] = value

    return cleaned_result


def search_with_perplexity(name: str, company: str) -> dict:
    """ä½¿ç”¨ Perplexity API é€²è¡Œæ·±åº¦æœå°‹ã€‚"""
    api_key = os.getenv("PERPLEXITY_API_KEY")

    if not api_key:
        print("    è­¦å‘Š: PERPLEXITY_API_KEY æœªè¨­å®š")
        return {}

    prompt = build_executive_search_prompt(name, company)

    system_prompt = """You are an elite Executive Search Researcher & Private Investigator.
CRITICAL RULES:
1. Age Heuristic: Calculate from graduation year if not found directly.
2. Zero Fabrication: NEVER guess contact info.
3. Executive Tone: Use Traditional Chinese (ç¹é«”ä¸­æ–‡).
Respond ONLY with valid JSON. No markdown, no explanations."""

    max_retries = 3
    for attempt in range(max_retries):
        try:
            response = requests.post(
                "https://api.perplexity.ai/chat/completions",
                headers={
                    "Authorization": f"Bearer {api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": "sonar-pro",
                    "messages": [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": prompt}
                    ],
                    "temperature": 0.1,
                    "max_tokens": 4000
                },
                timeout=120
            )

            if response.status_code == 200:
                result = response.json()
                content = result.get("choices", [{}])[0].get("message", {}).get("content", "")

                content = content.strip()
                if content.startswith("```json"):
                    content = content[7:]
                if content.startswith("```"):
                    content = content[3:]
                if content.endswith("```"):
                    content = content[:-3]
                content = content.strip()

                json_match = re.search(r'\{[\s\S]*\}', content)
                if json_match:
                    try:
                        api_data = json.loads(json_match.group())
                        excel_data = process_api_response(api_data)
                        found_fields = [k for k, v in excel_data.items() if v and not k.startswith("_")]
                        if found_fields:
                            print(f"    â†’ æ‰¾åˆ° {len(found_fields)} å€‹æ¬„ä½: {', '.join(found_fields)}")
                        return excel_data

                    except json.JSONDecodeError as e:
                        print(f"    JSON è§£æéŒ¯èª¤ ({attempt + 1}/{max_retries}): {e}")

            else:
                print(f"    Perplexity API éŒ¯èª¤ ({attempt + 1}/{max_retries}): {response.status_code}")
                if response.status_code == 429:
                    print("    â†’ API è«‹æ±‚éæ–¼é »ç¹ï¼Œç­‰å¾… 10 ç§’...")
                    time.sleep(10)

        except requests.exceptions.Timeout:
            print(f"    API è«‹æ±‚è¶…æ™‚ ({attempt + 1}/{max_retries})")
        except Exception as e:
            print(f"    æœå°‹éŒ¯èª¤ ({attempt + 1}/{max_retries}): {e}")

        if attempt < max_retries - 1:
            time.sleep(3)

    return {}


def multi_search_executive(name: str, company: str, missing_fields: list[str], search_client=None) -> dict:
    """ä½¿ç”¨å¤šé‡æœå°‹ç­–ç•¥ç²å–ä¸»ç®¡è³‡è¨Šã€‚"""
    result = {field: "" for field in missing_fields}

    use_unified = search_client is not None

    # === æœå°‹ç­–ç•¥ A: LinkedIn æª”æ¡ˆ ===
    print(f"    [ç­–ç•¥ A] æœå°‹ LinkedIn...")
    query_linkedin = f'"{name}" "{company}" LinkedIn'

    if use_unified:
        search_results_linkedin = search_client.search(query_linkedin, num_results=5)
    else:
        search_results_linkedin = search_with_ddg(query_linkedin, max_results=5)

    if search_results_linkedin:
        linkedin_info = extract_info_from_snippets(search_results_linkedin, name)
        for key, value in linkedin_info.items():
            if key in result and not result[key]:
                result[key] = value

        linkedin_url = extract_linkedin_url(search_results_linkedin)
        if linkedin_url:
            print(f"    â†’ æ‰¾åˆ° LinkedIn: {linkedin_url[:60]}...")

    time.sleep(1)

    # === æœå°‹ç­–ç•¥ B: ä¸­æ–‡ç°¡æ­·/ä»‹ç´¹ ===
    print(f"    [ç­–ç•¥ B] æœå°‹ä¸­æ–‡è³‡æ–™...")
    query_bio = f'"{name}" "{company}" ç°¡æ­· OR ä»‹ç´¹ OR ç¶“æ­· OR å­¸æ­·'

    if use_unified:
        search_results_bio = search_client.search(query_bio, num_results=5)
    else:
        search_results_bio = search_with_ddg(query_bio, max_results=5)

    if search_results_bio:
        bio_info = extract_info_from_snippets(search_results_bio, name)
        for key, value in bio_info.items():
            if key in result and not result[key]:
                result[key] = value

    time.sleep(1)

    # === æœå°‹ç­–ç•¥ C: Perplexity API ===
    still_missing = [f for f in missing_fields if not result.get(f)]

    if still_missing:
        print(f"    [ç­–ç•¥ C] Perplexity Executive Search Researcher...")
        print(f"    â†’ æœå°‹æ¬„ä½: {', '.join(still_missing)}")
        perplexity_result = search_with_perplexity(name, company)

        for key, value in perplexity_result.items():
            if key.startswith("_"):
                continue
            if key in result and not result[key] and value:
                result[key] = value

    time.sleep(1)

    # === æœå°‹ç­–ç•¥ D: Python ç«¯ç…§ç‰‡æœå°‹ ===
    photo_result = {"best_url": "", "best_score": 0, "status": "å¾…è£œå……", "candidates": []}

    if "ç…§ç‰‡" in missing_fields:
        print(f"    [ç­–ç•¥ D] Python ç«¯ç…§ç‰‡æœå°‹...")
        job_title = result.get("ç¾è·/ä»»", "")
        photo_result = find_executive_photo_python(name, company, str(job_title) if job_title else "")

        if photo_result["best_url"]:
            result["ç…§ç‰‡"] = photo_result["best_url"]
        result["ç…§ç‰‡ç‹€æ…‹"] = photo_result["status"]

    result["_photo_candidates"] = photo_result

    return result


def generate_photo_review_html(photo_data: dict):
    """ç”Ÿæˆç…§ç‰‡å¯©æ ¸ HTML å ±å‘Šã€‚"""
    html_content = """<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç…§ç‰‡å¯©æ ¸å ±å‘Š - CEO Project</title>
    <style>
        * { box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Microsoft JhengHei", sans-serif;
            background: #f5f5f5;
            margin: 0;
            padding: 20px;
        }
        .container { max-width: 1200px; margin: 0 auto; }
        h1 { color: #333; border-bottom: 3px solid #007bff; padding-bottom: 10px; }
        .instructions {
            background: #e7f3ff;
            border: 1px solid #b3d9ff;
            border-radius: 8px;
            padding: 15px;
            margin-bottom: 20px;
        }
        .instructions h3 { margin-top: 0; color: #0056b3; }
        .person-card {
            background: white;
            border-radius: 12px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
            margin-bottom: 20px;
            overflow: hidden;
        }
        .person-header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 15px 20px;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }
        .person-header h2 { margin: 0; font-size: 1.3em; }
        .status-badge {
            padding: 5px 12px;
            border-radius: 20px;
            font-size: 0.85em;
            font-weight: bold;
        }
        .status-pending { background: #ffc107; color: #333; }
        .status-confirm { background: #28a745; color: white; }
        .candidates-grid {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(200px, 1fr));
            gap: 15px;
            padding: 20px;
        }
        .candidate {
            border: 3px solid #ddd;
            border-radius: 8px;
            overflow: hidden;
            cursor: pointer;
            transition: all 0.2s;
            position: relative;
        }
        .candidate:hover { border-color: #007bff; transform: translateY(-2px); }
        .candidate.selected { border-color: #28a745; background: #e8f5e9; }
        .candidate.selected::after {
            content: "âœ“";
            position: absolute;
            top: 10px;
            right: 10px;
            background: #28a745;
            color: white;
            width: 30px;
            height: 30px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: bold;
        }
        .candidate img {
            width: 100%;
            height: 180px;
            object-fit: cover;
            display: block;
        }
        .candidate-info { padding: 10px; font-size: 0.85em; }
        .candidate-score {
            display: inline-block;
            padding: 2px 8px;
            border-radius: 10px;
            font-weight: bold;
            font-size: 0.8em;
        }
        .score-high { background: #d4edda; color: #155724; }
        .score-medium { background: #fff3cd; color: #856404; }
        .score-low { background: #f8d7da; color: #721c24; }
        .no-select {
            border: 3px dashed #ccc;
            display: flex;
            align-items: center;
            justify-content: center;
            min-height: 220px;
            color: #666;
            cursor: pointer;
        }
        .no-select:hover { border-color: #999; background: #f9f9f9; }
        .actions {
            padding: 15px 20px;
            background: #f8f9fa;
            border-top: 1px solid #eee;
        }
        .url-input {
            width: 100%;
            padding: 8px 12px;
            border: 1px solid #ddd;
            border-radius: 4px;
        }
        .btn {
            padding: 10px 20px;
            border: none;
            border-radius: 6px;
            cursor: pointer;
            font-weight: bold;
            margin-right: 10px;
        }
        .btn-success { background: #28a745; color: white; }
        .save-section {
            position: sticky;
            bottom: 0;
            background: white;
            padding: 20px;
            box-shadow: 0 -2px 10px rgba(0,0,0,0.1);
            text-align: center;
        }
        .img-error {
            background: #f8f9fa;
            display: flex;
            align-items: center;
            justify-content: center;
            height: 180px;
            color: #999;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸ“¸ ç…§ç‰‡å¯©æ ¸å ±å‘Š</h1>
        <div class="instructions">
            <h3>ä½¿ç”¨èªªæ˜</h3>
            <ol>
                <li>é»æ“Šæ­£ç¢ºçš„ç…§ç‰‡é¸æ“‡å®ƒ</li>
                <li>å¦‚æœæ‰€æœ‰ç…§ç‰‡éƒ½ä¸å°ï¼Œé»æ“Šã€Œéƒ½ä¸æ­£ç¢ºã€</li>
                <li>å®Œæˆå¾Œé»æ“Šã€Œå„²å­˜é¸æ“‡ã€æŒ‰éˆ•</li>
                <li>å°‡ä¸‹è¼‰çš„ JSON æª”æ¡ˆæ”¾åˆ° output/data/ è³‡æ–™å¤¾</li>
            </ol>
        </div>
        <div id="persons-container">
"""

    for row_str, data in sorted(photo_data.items(), key=lambda x: int(x[0])):
        name = data.get("name", "æœªçŸ¥")
        company = data.get("company", "")
        status = data.get("status", "å¾…è£œå……")
        best_url = data.get("best_url", "")
        candidates = data.get("candidates", [])

        status_class = "status-confirm" if status == "å¾…ç¢ºèª" else "status-pending"

        html_content += f"""
        <div class="person-card" data-row="{row_str}">
            <div class="person-header">
                <h2>[åˆ— {row_str}] {name} - {company}</h2>
                <span class="status-badge {status_class}">{status}</span>
            </div>
            <div class="candidates-grid">
"""

        for i, candidate in enumerate(candidates):
            url = candidate.get("url", "")
            score = candidate.get("score", 0)
            source = candidate.get("source", "")

            if score >= 40:
                score_class = "score-high"
            elif score >= 20:
                score_class = "score-medium"
            else:
                score_class = "score-low"

            selected_class = "selected" if url == best_url and best_url else ""

            html_content += f"""
                <div class="candidate {selected_class}" data-url="{url}" onclick="selectCandidate(this, '{row_str}')">
                    <img src="{url}" alt="å€™é¸ç…§ç‰‡ {i+1}" onerror="this.parentElement.innerHTML='<div class=img-error>åœ–ç‰‡è¼‰å…¥å¤±æ•—</div>'">
                    <div class="candidate-info">
                        <span class="candidate-score {score_class}">åˆ†æ•¸: {score}</span>
                    </div>
                </div>
"""

        html_content += f"""
                <div class="no-select" data-url="" onclick="selectCandidate(this, '{row_str}')">
                    <span>âŒ éƒ½ä¸æ­£ç¢º</span>
                </div>
            </div>
            <div class="actions">
                <label>æ‰‹å‹•è¼¸å…¥ç…§ç‰‡ URLï¼š</label>
                <input type="text" class="url-input" id="url-{row_str}" placeholder="è²¼ä¸Šæ­£ç¢ºçš„ç…§ç‰‡ URL...">
            </div>
        </div>
"""

    html_content += """
        </div>
        <div class="save-section">
            <button class="btn btn-success" onclick="saveSelections()">ğŸ’¾ å„²å­˜é¸æ“‡</button>
        </div>
    </div>
    <script>
        let selections = {};
        document.querySelectorAll('.person-card').forEach(card => {
            const row = card.dataset.row;
            const selected = card.querySelector('.candidate.selected');
            if (selected) selections[row] = selected.dataset.url;
        });

        function selectCandidate(element, row) {
            const card = element.closest('.person-card');
            card.querySelectorAll('.candidate, .no-select').forEach(c => c.classList.remove('selected'));
            element.classList.add('selected');
            selections[row] = element.dataset.url;
        }

        function saveSelections() {
            const output = {};
            document.querySelectorAll('.person-card').forEach(card => {
                const row = card.dataset.row;
                const manualUrl = document.getElementById('url-' + row).value.trim();
                output[row] = {
                    selected_url: manualUrl || selections[row] || '',
                    status: (manualUrl || selections[row]) ? 'å·²ç¢ºèª' : 'å¾…è£œå……'
                };
            });
            const blob = new Blob([JSON.stringify(output, null, 2)], {type: 'application/json'});
            const url = URL.createObjectURL(blob);
            const a = document.createElement('a');
            a.href = url;
            a.download = 'photo_selections.json';
            a.click();
        }
    </script>
</body>
</html>
"""

    html_path = Path(PHOTO_REVIEW_HTML)
    html_path.parent.mkdir(parents=True, exist_ok=True)
    with open(html_path, 'w', encoding='utf-8') as f:
        f.write(html_content)


def search_photos_only(rows_str: str):
    """åƒ…æœå°‹ç…§ç‰‡æ¨¡å¼ã€‚"""
    print("=" * 60)
    print("ç…§ç‰‡æœå°‹ç¨‹åºå•Ÿå‹• (Photos Only Mode)")
    print("=" * 60)

    # æ¸¬è©¦ç¶²è·¯é€£ç·š
    print("\næª¢æŸ¥ç¶²è·¯é€£ç·š...")
    if not test_network_connection():
        print("éŒ¯èª¤: ç¶²è·¯é€£ç·šç•°å¸¸ï¼Œè«‹æª¢æŸ¥ç¶²è·¯è¨­å®š")
        sys.exit(1)
    print("ç¶²è·¯é€£ç·šæ­£å¸¸")

    target_rows = parse_row_numbers(rows_str)
    if not target_rows:
        print("éŒ¯èª¤: æ²’æœ‰æœ‰æ•ˆçš„ç›®æ¨™åˆ—è™Ÿ")
        sys.exit(1)

    print(f"\nç›®æ¨™ Excel åˆ—è™Ÿ: {target_rows}")
    print(f"å…± {len(target_rows)} åˆ—å¾…è™•ç†")

    try:
        if Path(EXCEL_OUTPUT).exists():
            df = read_excel_safe(EXCEL_OUTPUT)
            print(f"\nè®€å– '{EXCEL_OUTPUT}'")
        else:
            df = read_excel_safe(EXCEL_INPUT)
            print(f"\nè®€å– '{EXCEL_INPUT}'")

        if "ç…§ç‰‡" not in df.columns:
            df["ç…§ç‰‡"] = None
        if "ç…§ç‰‡ç‹€æ…‹" not in df.columns:
            df["ç…§ç‰‡ç‹€æ…‹"] = None

        df["ç…§ç‰‡"] = df["ç…§ç‰‡"].astype(object)
        df["ç…§ç‰‡ç‹€æ…‹"] = df["ç…§ç‰‡ç‹€æ…‹"].astype(object)

    except FileNotFoundError:
        print(f"éŒ¯èª¤: æ‰¾ä¸åˆ° Excel æª”æ¡ˆ")
        sys.exit(1)
    except Exception as e:
        print(f"éŒ¯èª¤: è®€å– Excel å¤±æ•— - {e}")
        sys.exit(1)

    max_excel_row = len(df) + 1
    invalid_rows = [r for r in target_rows if r > max_excel_row or r < 2]
    if invalid_rows:
        print(f"è­¦å‘Š: ä»¥ä¸‹åˆ—è™Ÿè¶…å‡ºç¯„åœ: {invalid_rows}")
        target_rows = [r for r in target_rows if r <= max_excel_row and r >= 2]

    if not target_rows:
        print("éŒ¯èª¤: æ²’æœ‰æœ‰æ•ˆçš„ç›®æ¨™åˆ—è™Ÿ")
        sys.exit(1)

    existing_photo_candidates = {}
    if Path(PHOTO_CANDIDATES_JSON).exists():
        try:
            with open(PHOTO_CANDIDATES_JSON, 'r', encoding='utf-8') as f:
                existing_photo_candidates = json.load(f)
        except:
            pass

    all_photo_candidates = {}
    updated_count = 0

    for excel_row in target_rows:
        pandas_idx = excel_row_to_pandas_index(excel_row)
        row_data = df.iloc[pandas_idx]

        name = row_data.get("å§“åï¼ˆä¸­è‹±ï¼‰", "")
        company = row_data.get("æ‰€å±¬å…¬å¸", "")
        job_title = row_data.get("ç¾è·/ä»»", "")

        if pd.isna(name) or not name:
            print(f"\n[åˆ— {excel_row}] è·³é - ç„¡å§“åè³‡æ–™")
            continue

        print(f"\n[åˆ— {excel_row}] æœå°‹ç…§ç‰‡: {name} ({company})")
        print("-" * 50)

        photo_result = find_executive_photo_python(name, company, str(job_title) if pd.notna(job_title) else "")

        if photo_result.get("candidates"):
            all_photo_candidates[excel_row] = {
                "name": name,
                "company": company,
                "best_url": photo_result.get("best_url", ""),
                "best_score": photo_result.get("best_score", 0),
                "status": photo_result.get("status", "å¾…è£œå……"),
                "candidates": photo_result.get("candidates", [])
            }

            if photo_result["best_url"]:
                df.at[pandas_idx, "ç…§ç‰‡"] = photo_result["best_url"]
                updated_count += 1
            df.at[pandas_idx, "ç…§ç‰‡ç‹€æ…‹"] = photo_result["status"]

        time.sleep(2)

    output_path = Path(EXCEL_OUTPUT)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    for col in ENRICHABLE_COLUMNS:
        if col in df.columns:
            df[col] = df[col].apply(lambda x: _clean_value(x) if pd.notna(x) else "")

    try:
        df.to_excel(output_path, index=False, engine='openpyxl')
        print(f"\n{'=' * 60}")
        print(f"ç…§ç‰‡æœå°‹å®Œæˆï¼")
        print(f"  - è™•ç†åˆ—æ•¸: {len(target_rows)}")
        print(f"  - æ‰¾åˆ°ç…§ç‰‡: {updated_count} ç­†")
        print(f"  - è¼¸å‡ºæª”æ¡ˆ: {output_path}")
    except PermissionError:
        backup_path = output_path.with_name("Standard_Example_Enriched_backup.xlsx")
        try:
            df.to_excel(backup_path, index=False, engine='openpyxl')
            print(f"\nâš ï¸  åŸæª”æ¡ˆè¢«é–å®šï¼Œå·²å„²å­˜åˆ°: {backup_path}")
        except Exception as e2:
            print(f"éŒ¯èª¤: å„²å­˜ Excel å¤±æ•— - {e2}")
            sys.exit(1)
    except Exception as e:
        print(f"éŒ¯èª¤: å„²å­˜ Excel å¤±æ•— - {e}")
        sys.exit(1)

    if all_photo_candidates:
        for row, data in all_photo_candidates.items():
            existing_photo_candidates[str(row)] = data

        json_path = Path(PHOTO_CANDIDATES_JSON)
        try:
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(existing_photo_candidates, f, ensure_ascii=False, indent=2)
            print(f"\nç…§ç‰‡å€™é¸è³‡æ–™å·²å„²å­˜: {json_path}")
        except Exception as e:
            print(f"è­¦å‘Š: å„²å­˜ç…§ç‰‡å€™é¸ JSON å¤±æ•— - {e}")

        try:
            generate_photo_review_html(existing_photo_candidates)
            print(f"ç…§ç‰‡å¯©æ ¸å ±å‘Šå·²ç”Ÿæˆ: {PHOTO_REVIEW_HTML}")
        except Exception as e:
            print(f"è­¦å‘Š: ç”Ÿæˆç…§ç‰‡å¯©æ ¸å ±å‘Šå¤±æ•— - {e}")

    print(f"\n{'=' * 60}")


def enrich_data(rows_str: str, photos_only: bool = False):
    """ä¸»è¦è³‡æ–™æ“´å……å‡½å¼ã€‚"""
    if photos_only:
        search_photos_only(rows_str)
        return

    print("=" * 60)
    print("è³‡æ–™æ“´å……ç¨‹åºå•Ÿå‹• (Executive Search Researcher Quality)")
    print("=" * 60)

    # æ¸¬è©¦ç¶²è·¯é€£ç·š
    print("\næª¢æŸ¥ç¶²è·¯é€£ç·š...")
    if not test_network_connection():
        print("è­¦å‘Š: ç¶²è·¯é€£ç·šå¯èƒ½æœ‰å•é¡Œï¼Œç¹¼çºŒåŸ·è¡Œä½†å¯èƒ½æœƒå¤±æ•—...")
    else:
        print("ç¶²è·¯é€£ç·šæ­£å¸¸")

    search_client = None
    if UNIFIED_SEARCH_AVAILABLE:
        search_client = UnifiedSearchClient()
        status = search_client.get_status()
        print("\næœå°‹å¼•æ“ç‹€æ…‹:")
        print(f"  ä¸»è¦å¼•æ“: {status['primary_engine']}")
    else:
        print("\næœå°‹å¼•æ“ç‹€æ…‹:")
        print("  ä½¿ç”¨: DuckDuckGo (ç›´æ¥æ¨¡å¼)")

    target_rows = parse_row_numbers(rows_str)
    if not target_rows:
        print("éŒ¯èª¤: æ²’æœ‰æœ‰æ•ˆçš„ç›®æ¨™åˆ—è™Ÿ")
        sys.exit(1)

    print(f"\nç›®æ¨™ Excel åˆ—è™Ÿ: {target_rows}")
    print(f"å…± {len(target_rows)} åˆ—å¾…è™•ç†")

    try:
        if Path(EXCEL_OUTPUT).exists():
            df = read_excel_safe(EXCEL_OUTPUT)
            print(f"\nè®€å– '{EXCEL_OUTPUT}'")
        else:
            df = read_excel_safe(EXCEL_INPUT)
            print(f"\nè®€å– '{EXCEL_INPUT}'")

        print(f"è³‡æ–™çµæ§‹: {len(df)} åˆ— x {len(df.columns)} æ¬„")

        for col in ENRICHABLE_COLUMNS + ["ç…§ç‰‡ç‹€æ…‹", "å°ˆæ¥­åˆ†é¡"]:
            if col not in df.columns:
                df[col] = None
            df[col] = df[col].astype(object)

    except FileNotFoundError:
        print(f"éŒ¯èª¤: æ‰¾ä¸åˆ° Excel æª”æ¡ˆ")
        sys.exit(1)
    except Exception as e:
        print(f"éŒ¯èª¤: è®€å– Excel å¤±æ•— - {e}")
        sys.exit(1)

    max_excel_row = len(df) + 1
    invalid_rows = [r for r in target_rows if r > max_excel_row or r < 2]
    if invalid_rows:
        print(f"è­¦å‘Š: ä»¥ä¸‹åˆ—è™Ÿè¶…å‡ºç¯„åœ: {invalid_rows}")
        target_rows = [r for r in target_rows if r <= max_excel_row and r >= 2]

    if not target_rows:
        print("éŒ¯èª¤: æ²’æœ‰æœ‰æ•ˆçš„ç›®æ¨™åˆ—è™Ÿ")
        sys.exit(1)

    updated_count = 0
    total_fields = 0
    all_photo_candidates = {}

    for excel_row in target_rows:
        pandas_idx = excel_row_to_pandas_index(excel_row)
        row_data = df.iloc[pandas_idx]

        name = row_data.get("å§“åï¼ˆä¸­è‹±ï¼‰", "")
        company = row_data.get("æ‰€å±¬å…¬å¸", "")

        if pd.isna(name) or not name:
            print(f"\n[åˆ— {excel_row}] è·³é - ç„¡å§“åè³‡æ–™")
            continue

        print(f"\n[åˆ— {excel_row}] è™•ç†ä¸­: {name} ({company})")
        print("-" * 50)

        missing_fields = []
        for col in ENRICHABLE_COLUMNS:
            if col in df.columns:
                val = row_data.get(col)
                if pd.isna(val) or val == "" or val == 0:
                    missing_fields.append(col)

        if not missing_fields:
            print(f"  â†’ æ‰€æœ‰æ¬„ä½å·²æœ‰è³‡æ–™ï¼Œè·³é")
            continue

        print(f"  ç©ºç¼ºæ¬„ä½ ({len(missing_fields)}): {', '.join(missing_fields)}")
        total_fields += len(missing_fields)

        found_data = multi_search_executive(name, company, missing_fields, search_client)

        photo_candidates_info = found_data.pop("_photo_candidates", None)
        if photo_candidates_info and photo_candidates_info.get("candidates"):
            all_photo_candidates[excel_row] = {
                "name": name,
                "company": company,
                "best_url": photo_candidates_info.get("best_url", ""),
                "best_score": photo_candidates_info.get("best_score", 0),
                "status": photo_candidates_info.get("status", "å¾…è£œå……"),
                "candidates": photo_candidates_info.get("candidates", [])
            }

        fields_filled = 0
        for field, value in found_data.items():
            if field.startswith("_"):
                continue
            if field in df.columns and value:
                if field in missing_fields or field == "ç…§ç‰‡ç‹€æ…‹":
                    df.at[pandas_idx, field] = value

                    display_value = str(value).replace('\n', ' | ')
                    if len(display_value) > 60:
                        display_value = display_value[:60] + "..."
                    print(f"  âœ“ [{field}]: {display_value}")

                    if field in missing_fields:
                        updated_count += 1
                        fields_filled += 1

        print(f"\n  â†’ æœ¬åˆ—å¡«å…¥ {fields_filled}/{len(missing_fields)} å€‹æ¬„ä½")
        time.sleep(2)

    output_path = Path(EXCEL_OUTPUT)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    for col in ENRICHABLE_COLUMNS:
        if col in df.columns:
            df[col] = df[col].apply(lambda x: _clean_value(x) if pd.notna(x) else "")

    try:
        df.to_excel(output_path, index=False, engine='openpyxl')
        print(f"\n{'=' * 60}")
        print(f"æ“´å……å®Œæˆï¼")
        print(f"  - è™•ç†åˆ—æ•¸: {len(target_rows)}")
        print(f"  - ç¸½ç©ºç¼ºæ¬„ä½: {total_fields}")
        print(f"  - æˆåŠŸå¡«å…¥æ¬„ä½: {updated_count}")
        print(f"  - å¡«å…¥ç‡: {updated_count/total_fields*100:.1f}%" if total_fields > 0 else "  - å¡«å…¥ç‡: N/A")
        print(f"  - è¼¸å‡ºæª”æ¡ˆ: {output_path}")
        print(f"{'=' * 60}")
    except Exception as e:
        print(f"éŒ¯èª¤: å„²å­˜ Excel å¤±æ•— - {e}")
        sys.exit(1)

    if all_photo_candidates:
        json_path = Path(PHOTO_CANDIDATES_JSON)
        try:
            existing_data = {}
            if json_path.exists():
                with open(json_path, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)

            for row, data in all_photo_candidates.items():
                existing_data[str(row)] = data

            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(existing_data, f, ensure_ascii=False, indent=2)

            print(f"\nç…§ç‰‡å€™é¸è³‡æ–™å·²å„²å­˜: {json_path}")
        except Exception as e:
            print(f"è­¦å‘Š: å„²å­˜ç…§ç‰‡å€™é¸ JSON å¤±æ•— - {e}")

        try:
            generate_photo_review_html(existing_data if existing_data else all_photo_candidates)
            print(f"ç…§ç‰‡å¯©æ ¸å ±å‘Šå·²ç”Ÿæˆ: {PHOTO_REVIEW_HTML}")
        except Exception as e:
            print(f"è­¦å‘Š: ç”Ÿæˆç…§ç‰‡å¯©æ ¸å ±å‘Šå¤±æ•— - {e}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="è³‡æ–™æ“´å……è…³æœ¬ (Executive Search Researcher Quality)",
    )
    parser.add_argument(
        "--rows",
        type=str,
        required=True,
        help="è¦è™•ç†çš„ Excel åˆ—è™Ÿ"
    )
    parser.add_argument(
        "--photos-only",
        action="store_true",
        help="åƒ…æœå°‹ç…§ç‰‡"
    )

    args = parser.parse_args()
    enrich_data(args.rows, photos_only=args.photos_only)
